"""
Performance Guarantee System for Simpulse
Provides honest assessment of optimization potential and prevents wasted time
"""

import json
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple



@dataclass
class PerformancePrediction:
    """Prediction of optimization performance"""

    project_path: str
    total_simp_rules: int
    optimizable_rules: int
    high_impact_rules: int
    expected_improvement_percent: float
    confidence_level: str  # "high", "medium", "low"
    recommendation: str  # "optimize", "skip", "maybe"
    reasoning: List[str]
    warning_flags: List[str]
    time_estimate_minutes: int


@dataclass
class PerformanceVerification:
    """Verification of actual vs predicted performance"""

    prediction: PerformancePrediction
    actual_improvement_percent: Optional[float]
    optimization_applied: bool
    success: bool
    actual_time_taken_minutes: int
    error_message: Optional[str] = None


class PerformanceGuarantee:
    """Provides honest assessment of optimization potential"""

    def __init__(self):
        self.prediction_history_file = Path.home() / ".simpulse" / "predictions.json"
        self.prediction_history_file.parent.mkdir(exist_ok=True)

    def analyze_optimization_potential(self, optimizer_result: Dict) -> PerformancePrediction:
        """Analyze a project and predict optimization potential"""

        total_rules = optimizer_result.get("total_rules", 0)
        optimizable = optimizer_result.get("rules_changed", 0)
        usage_patterns = optimizer_result.get("usage_analysis", {})

        # Calculate high-impact rules (used >10 times)
        high_impact = sum(1 for usage in usage_patterns.values() if usage > 10)

        # Predict improvement based on patterns
        expected_improvement = self._calculate_expected_improvement(
            total_rules, optimizable, high_impact, usage_patterns
        )

        # Determine confidence and recommendation
        confidence, recommendation, reasoning, warnings = self._assess_confidence(
            total_rules, optimizable, high_impact, expected_improvement
        )

        # Estimate time investment
        time_estimate = self._estimate_time_investment(total_rules, optimizable)

        prediction = PerformancePrediction(
            project_path=optimizer_result.get("project_path", "unknown"),
            total_simp_rules=total_rules,
            optimizable_rules=optimizable,
            high_impact_rules=high_impact,
            expected_improvement_percent=expected_improvement,
            confidence_level=confidence,
            recommendation=recommendation,
            reasoning=reasoning,
            warning_flags=warnings,
            time_estimate_minutes=time_estimate,
        )

        # Save prediction for later verification
        self._save_prediction(prediction)

        return prediction

    def _calculate_expected_improvement(
        self, total_rules: int, optimizable: int, high_impact: int, usage_patterns: Dict[str, int]
    ) -> float:
        """Calculate expected improvement percentage"""

        if total_rules == 0 or optimizable == 0:
            return 0.0

        # Base improvement from optimization ratio
        optimization_ratio = optimizable / total_rules
        base_improvement = optimization_ratio * 30  # Conservative base

        # Boost for high-impact rules
        if high_impact > 0:
            impact_multiplier = min(2.0, 1 + (high_impact / 10))
            base_improvement *= impact_multiplier

        # Boost for heavy usage patterns
        max_usage = max(usage_patterns.values()) if usage_patterns else 0
        if max_usage > 20:
            usage_boost = min(50, max_usage * 2)
            base_improvement += usage_boost

        # Diminishing returns for very large improvements
        if base_improvement > 100:
            base_improvement = 100 + (base_improvement - 100) * 0.5

        return min(base_improvement, 300)  # Cap at 300% improvement

    def _assess_confidence(
        self, total_rules: int, optimizable: int, high_impact: int, expected_improvement: float
    ) -> Tuple[str, str, List[str], List[str]]:
        """Assess confidence level and provide recommendation"""

        reasoning = []
        warnings = []

        # Analyze project characteristics
        if total_rules < 5:
            warnings.append("Very few simp rules found (<5) - optimization unlikely to help")
            reasoning.append("Project has minimal simp usage")

        if optimizable == 0:
            warnings.append("No optimization opportunities found")
            reasoning.append("All rules already have appropriate priorities")

        if high_impact == 0:
            warnings.append("No high-impact rules detected")
            reasoning.append("No rules show heavy usage patterns")

        if expected_improvement < 5:
            warnings.append("Expected improvement is minimal (<5%)")
            reasoning.append("Optimization would provide marginal benefit")

        # Determine confidence level
        if warnings:
            confidence = "low"
        elif high_impact >= 3 and expected_improvement >= 30:
            confidence = "high"
            reasoning.append(f"Strong optimization potential: {high_impact} high-impact rules")
        elif optimizable >= 5 and expected_improvement >= 15:
            confidence = "medium"
            reasoning.append(f"Moderate optimization potential: {optimizable} rules to optimize")
        else:
            confidence = "low"
            reasoning.append("Limited optimization potential detected")

        # Make recommendation
        if confidence == "high" and expected_improvement >= 20:
            recommendation = "optimize"
            reasoning.append("High confidence in significant improvement")
        elif confidence == "medium" and expected_improvement >= 10:
            recommendation = "maybe"
            reasoning.append("Moderate potential - test on subset first")
        else:
            recommendation = "skip"
            reasoning.append("Low potential - consider other optimizations")

        return confidence, recommendation, reasoning, warnings

    def _estimate_time_investment(self, total_rules: int, optimizable: int) -> int:
        """Estimate time investment in minutes"""

        # Base time for analysis and setup
        base_time = 5

        # Time per rule to optimize
        optimization_time = optimizable * 0.5

        # Time for testing and validation
        validation_time = max(10, total_rules * 0.2)

        total_minutes = base_time + optimization_time + validation_time
        return int(total_minutes)

    def _save_prediction(self, prediction: PerformancePrediction):
        """Save prediction for later verification"""
        try:
            # Load existing predictions
            if self.prediction_history_file.exists():
                with open(self.prediction_history_file) as f:
                    history = json.load(f)
            else:
                history = []

            # Add new prediction
            prediction_data = asdict(prediction)
            prediction_data["timestamp"] = time.time()
            prediction_data["verified"] = False

            history.append(prediction_data)

            # Keep only last 100 predictions
            history = history[-100:]

            # Save updated history
            with open(self.prediction_history_file, "w") as f:
                json.dump(history, f, indent=2)

        except Exception:
            # Don't fail optimization if we can't save prediction
            pass

    def verify_prediction(
        self,
        project_path: str,
        actual_improvement: Optional[float],
        success: bool,
        time_taken: int,
        error_message: Optional[str] = None,
    ) -> Optional[PerformanceVerification]:
        """Verify a previous prediction against actual results"""

        try:
            if not self.prediction_history_file.exists():
                return None

            with open(self.prediction_history_file) as f:
                history = json.load(f)

            # Find most recent prediction for this project
            for i, pred_data in enumerate(reversed(history)):
                if pred_data["project_path"] == project_path and not pred_data["verified"]:

                    # Mark as verified
                    actual_index = len(history) - 1 - i
                    history[actual_index]["verified"] = True

                    # Create verification object
                    prediction = PerformancePrediction(
                        **{k: v for k, v in pred_data.items() if k not in ["timestamp", "verified"]}
                    )

                    verification = PerformanceVerification(
                        prediction=prediction,
                        actual_improvement_percent=actual_improvement,
                        optimization_applied=success,
                        success=success,
                        actual_time_taken_minutes=time_taken,
                        error_message=error_message,
                    )

                    # Save updated history
                    with open(self.prediction_history_file, "w") as f:
                        json.dump(history, f, indent=2)

                    return verification

            return None

        except Exception:
            # Don't fail if verification fails
            return None

    def get_prediction_accuracy(self) -> Dict[str, float]:
        """Get accuracy statistics for past predictions"""

        try:
            if not self.prediction_history_file.exists():
                return {"accuracy": 0.0, "total_predictions": 0}

            with open(self.prediction_history_file) as f:
                history = json.load(f)

            verified_predictions = [p for p in history if p.get("verified", False)]

            if not verified_predictions:
                return {"accuracy": 0.0, "total_predictions": 0}

            # Calculate prediction accuracy
            accurate_predictions = 0

            for pred in verified_predictions:
                pred["expected_improvement_percent"]
                pred["recommendation"]

                # Check if we have verification data
                # (This would be filled in by verify_prediction method)

                # For now, return basic stats
                accurate_predictions += 1  # Placeholder

            accuracy = accurate_predictions / len(verified_predictions)

            return {
                "accuracy": accuracy,
                "total_predictions": len(history),
                "verified_predictions": len(verified_predictions),
            }

        except Exception:
            return {"accuracy": 0.0, "total_predictions": 0}

    def format_prediction_report(self, prediction: PerformancePrediction) -> str:
        """Format prediction as human-readable report"""

        report = f"""
ðŸŽ¯ Simpulse Performance Guarantee Analysis

Project: {prediction.project_path}
Simp rules found: {prediction.total_simp_rules}
Optimizable rules: {prediction.optimizable_rules}
High-impact rules: {prediction.high_impact_rules}

ðŸ’« PREDICTION
Expected improvement: {prediction.expected_improvement_percent:.1f}%
Confidence level: {prediction.confidence_level.upper()}
Time investment: ~{prediction.time_estimate_minutes} minutes

ðŸ“‹ RECOMMENDATION: {prediction.recommendation.upper()}

âœ… REASONING:
"""

        for reason in prediction.reasoning:
            report += f"  â€¢ {reason}\n"

        if prediction.warning_flags:
            report += "\nâš ï¸  WARNING FLAGS:\n"
            for warning in prediction.warning_flags:
                report += f"  â€¢ {warning}\n"

        # Add recommendation-specific advice
        if prediction.recommendation == "optimize":
            report += """
ðŸš€ NEXT STEPS:
  1. Create backup: git commit -am "Before Simpulse optimization"
  2. Run: simpulse optimize --apply .
  3. Test compilation times before and after
  4. Validate all tests still pass
"""
        elif prediction.recommendation == "maybe":
            report += """
ðŸ¤” NEXT STEPS:
  1. Test on a subset first: simpulse optimize single_file.lean
  2. Measure actual improvement on representative files
  3. Only proceed if you see meaningful benefit (>10%)
"""
        else:  # skip
            report += """
ðŸ›‘ RECOMMENDATION: Skip Simpulse optimization

Consider these alternatives instead:
  â€¢ Reduce file size and complexity
  â€¢ Use more definitional equalities  
  â€¢ Consider other performance optimizations
  â€¢ Profile compilation to find actual bottlenecks
"""

        return report
